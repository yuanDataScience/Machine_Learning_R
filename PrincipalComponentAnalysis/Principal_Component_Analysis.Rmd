---
title: 'Principal Component Analysis'
author: 'Yuan Huang'
output:
  html_notebook: default
  html_document: default
  pdf_document: default
---

### Introduction

This project implemented principal component analysis using R. This markdown file read the data files and organized functions defined in the function list. The corresponding notebook and the markdown files also included the derivation of the basic equations used for the implementation of the algorithms.    

#### Functions implemented in this markdown notebook
* PCA - Perform principal component analysis
* projectData - Projects a data set into a lower dimensional space
* recoverData - Recovers the original data from the projection
* featureNormalization - Normalize the features in data matrix(feature matrix), returns the mean and standard deviation of features
* drawPCAPlot - visualize the first two principle components and the 2-D data points
* plot_original_recovered_data - plot the original data points and data points recovered from low dimensional projection
* drawPVE - plot the Proportion of Variance Explained (PVE) plots, including scree plot and cummulative PVE plot

Before we implement the functions, we first load the R libraries needed for this project. 
```{r}
#load the library
library("ggplot2")
library("readr")
library(ISLR)
library(gridExtra)

```

**1. Principal Component Analysis (PCA) **              
This project implemented PCA to perform dimension reduction. PCA is an unsupervised method widely applied to the high dimensional data anaysis. when original dataset contains a large set of correclated features, principal compnents allow us to represent the data with a smaller number of "summarized" features, while still maintain and explain large portion of the vaiability in the original dataset. PCA provides a tool not only to reduce the dimensionality of the data for more efficient data storage and data compression, but also to visualize high dimensioal data using low-dimensional representation of the data.

**1.1. A simple 2D example **
Let's start from a simple 2D example of data. In the following R chunk, I created a 2D dataset, where the x and y variables are highly corrected. 
```{r}
par(mfrow=c(1,2))
x<-runif(100)
y<-2*x
plot(x,y)
projected<-cbind(x,y)%*%c(1/sqrt(5),2/sqrt(5))
plot(x=projected,y=rep(0,100),ylim=c(0,1),xlab="x",ylab="y")
```
The data originally generated is plotted in the left side of the figure. It is apparent that all the data points are aligned along a straight line. In this exteme case, if we can rotate the data points to make them aligned with the x axsis, as shown in the right side of the figure, then we will only need one dimension to represent the data, while still maintain the distances between the data points.

This is the basic idea of PCA: by projecting the original data onto a set of principal components, we present the data in a much lower dimensional space. You can consider principal components as a set of normalized vectors. These vectors are chosen so that when projecting the original data onto these vectors, the relative distances between the objects are best reserved. More specifically, for each principal components, we want the projected values to have the maximum variance. In addition, principal components are ordered according to the variances of their projected data. Variance of the projected data for the first principal component (PC1) is the largerest among all the principal components, and the variance of the projected data for PC2 is the second largest, and so on.

**1.2. mathematics of PCA **                 
**1.2.1. Definitions of symbols and matrices **                                  
PCA can be formulated as a mathematical maximization problem with constraints. In this section, I will define the symbols and terminologies used in the following derivation:                  
(a). data matrix X                      
Data matrix is a m by n matrix, where m is the nunmber of observations and n is the number of the features. The i-th row of X matrix, $X^{(i)}\in R^{(n)}$ is the feature vector corresponding to the i-th observation.

(b). loading matrix P                
Loading matrix P is a n by k matrix, where n is the number of features, and k is the number of principal components. The maximum value of k is determined by the rank of data matrix X. The k-th column of P, expressed as $p_k$, refers to the k-th principal component. Since principal components are orthonormal vectors, $p_i^Tp_j=0$ for $i\neq j$, and $p_i^Tp_i=1$ 

(c). score matrix T                
Score matrix T is a m by k matrix, where m is the number of observations and k is the number of the principal components. T matrix is generated by projecting data matrix X onto the k principal components contained in P matrix. Therefore, $T=XP$. The i-th row, j-th column element of T, $t_{i,j}$ is the score value of projecting the i-th observation onto the j-th principal component, $t_{(i,j)}=(X^{(i)})^TP_j$. In the following discussion, I will use $t_j$ to represent the j-th column of T matrix, which is a n dimensional vector. Each element in $t_j$ corresponds to the score of one observation on the j-th principal component. Therefore, $t_j=XP_j$. For example, $t_1$ is the vector containing the scores the n observations by projecting the observations onto the first principal component, and $t_1=XP_1$. An important property of T matrix is that score vectors corresponding to different principal components are orthogonal to each other. Therefore, $t_i^Tt_j=0$ for $i \neq j$. 

**1.2.2. A brief mathematic derivation **              
As mentioned previously, the principal components, $p_j$, $j\in$ {1,k} maximize the variance of the projected values of data matrix. As we know, the projected values of data matrix for principal component $p_k$ is the score vector $t_k$. Let's start from the first principal component, $p_1$. The score vector is the first column of score matrix T, $t_1$. For PCA, all the columns in data matrix X are centered and normalized. Therefore, the variance of $t_1$ is:
$$var(t_1)=t_1^Tt_1=(Xp_1)^TXp_1=p_1^TX^TXp_1=p_1^T\sum p_1$$
where $\sum=X^TX$ is the covariance matrix of the data matrix. In fact, this equation applies to all the principal components, since all the principal components are chosen to maximize the variance of their scores. Therefore, we have
$$var(t_j)=t_j^Tt_j=(Xp_j)^TXp_j=p_j^TX^TXp_j=p_j^T\sum p_j,~~~ j=1,...k ~~~~~~~~~~~Eq(1) $$

To find $t_j$ values that maximize $var(t_j)$ in Eq(1), and considering that $p_j$ are normalized vectors and therefore $p_j^Tp_j=1$, the lagrangian are
$$p_j^T\sum p_j -\lambda_j(p_j^T p_j -1) ,~~~ j=1,...k ~~~~~~~~~~~~~~~~~~~~~~~~Eq(2)$$
with the lagrange parameters $\lambda_j$. The solution is found by calculating the derivative of Eq(2) with respect to $t_j$, which gives us
$\sum p_j-\lambda_j p_j=0$, leading to 
$$\sum p_j = \lambda_j p_j ~~~~~~~~~~~~~~Eq(3)$$
Therefore, $p_j$ are the eigenvectors of the covariance matrix $\sum$. Furthermore, according to Eqs(1) and (3), 
$$var(t_j)=p_j^T\sum p_j = p_j^T \lambda_j p_j = \lambda_j p_j^Tp_j =\lambda_j ~~~~~~~ Eq(4)$$
As mentioned previously, the pricinpal components are ordered according to the variances of their corresoponding scores. Therefore, we know that the loading matrix is actually the eigenvector matrix, with the first column $p_1$, as the eigenvector corresponding to the largest eigenvalue, $\lambda_1$, and so on.

**1.2.3. Singular value decomposition **              
According to section 1.2.2, we can define the loading matrix P by the eigenvectors of the covariance matrix $X^TX$, and then define score matrix T using $T=XP$. In R, we can directly use the command svd(X) to find the loading and score matrix. The svd(X) command returns three variables: v, which is the loading marix. Each column of v corresponds to a principal component. d contains the square root of the eigenvalues. Another vector svd(X) returns is the vector u, and the product of $u_id_i$ is the corresponding $t_i$ in score matrix. 

**1.2.4. Reconstruction of data matrix from score and loading matrix **                
A commonly used concept in PCA is to reconstruct data matrix from score and loading matrices. Let's start from the simplest situation where all the loading vectors (or you can say, all the principal components) are used. Then according to $T=XP$, and the fact that columns $p_j$ in P matrix for j=1,...K are orthonormal, we have:
$$TP^T=XPP^T=X$$
If we only use the first j principle components and define $X_{app}=TP_{1,...j}^T$, we have
$$X=T_{1,..j}P_{1,..j}^T+E=X_{app}+E  ~~~~~~~~(j<K)~~~~~~~~~~~~Eq(5)$$
in Eq(5), K is the maximum number of pricipal components and E is the error(residual) marix that defines the difference between the reconstructed data matrix $X_{app}$ using part of the principal compoents and the original data matrix X. When all the loading vectors in P are used, E=0.  

**2. Practice of PCA **                             
**2.1. A siample 2D example **                      
We will start from reducing data in 2-D to 1-D using PCA. First, let's load the 2D data and explore the data.
```{r}
data1<- read.csv("ex7data1X.txt",header=FALSE)
df.data1<-as.data.frame(data1)

ggplot(df.data1,aes(x=df.data1[,1],y=df.data1[,2]))+geom_point(size=3)
```
We can see that the two dimensions of the data are correlated to some extents.

**2.2. Implementing PCA**                                             

Before we use the svd() command to decompose data matrix X and get the score and loading matrices, we first need to normalize the data matrix. This is because the direction of each principal component is selected to maximize the variances of their scores. if some features have much larger magnitude than the others, then these features will dominate the directions of the principal components, and we will not be able to see the effects of the features having small scales. Therefore, we will scale the data matrix so that all the features will have the same scale. In addition, by centering the data matrix features, we can conveniently compute covariance matrix from the centered data matrix X using $X^TX$.

By normalizing the feature matrix, first, each column is subtracted by its column mean, and then divided by its column standard deviation. In the follwoing R chunk, function featureNormalization() was implemented to normalize the feature matrix.
```{r}
featureNormalization<-function(X){
  mean<-apply(X,2,mean)
  sigma<-apply(X,2,sd)
  X_norm<-scale(X)
  return(list(mu=mean,sigma=sigma,X_norm=X_norm))
}

normalized_data1<-featureNormalization(data1)
```
After data normalization, we can use the svd() command to decompose the data matrix X to obtain the principal components in loading marix P and score matix: 
```{r}
PCA<-function(X){
  X<-as.matrix(X)
  X<-scale(X)
  return(svd(X))
}

```
The following R chunk defines the function drawPCAPlot() that normalizes feature matrix, and performs the svd decomposition for PCA analysis. It also visualizes the first two principle components.
```{r}
drawPCAPlot<-function(X){
  X<-as.matrix(X)
  df_X<-as.data.frame(X)
  normData<-featureNormalization(X)
  PCA_result<-PCA(normData$X_norm)
  
  U<-PCA_result$v
  S<-PCA_result$d
  s_total<-sum(S)
  mu<-normData$mu
  pca_points<-rbind(mu,mu+3*S[1]*U[,1]/s_total,mu,mu+3*S[2]*U[,2]/s_total)
  group_vector<-c(1,1,2,2)
  pca_points<-as.data.frame(cbind(pca_points,group_vector))
  ggplot(data=df_X, aes(x=df_X[,1],y=df_X[,2]))+geom_point(col="red")+
    geom_line(data=pca_points,aes(x=pca_points[,1],y=pca_points[,2],group=pca_points[,3]),col="blue")+
    xlab("X")+ylab("Y")
}

drawPCAPlot(data1)
```
From this figure, the two principle components are orthogonal to each other.

**2.3. Dimensionality Reduction with PCA **                                        
In this part, we practice the dimensional reduction by projecting the original data onto the first k principal components, and using the projected data to represent the original data. By doing this, we reduce the dimension of the original data to the k-dimensional data. The following R chunk implemented projectData() function that projects the data onto the first k principal components.

```{r}
projectData<-function(X,U,k){
  X<-as.matrix(X)
  U<-as.matrix(U)
  
  U_reduced<-U[,1:k]
  return(X%*%U_reduced)
}
```
In the following R code, features in data1 were first normalized, and decomposed by singular value decomposition using the function PCA(). The pricinpal components obtained by PCA were stored in data1_U matrix. projectData() function was then used to project the normalized data, data1_norm onto the first principal compnent, and the projected score for the first example was printed out. This code chunk showed how the two dimensional data was reduced to one dimsion. 

```{r}
data1_norm<-featureNormalization(data1)$X_norm
data1_V <-PCA(data1)$v

proj_data1 <- projectData(data1_norm,data1_V,1)
cat("project the first experiment ",proj_data1[1,1],"\n")
cat("the value should be 1.481274 or -1.48127")
```
In fact, you can obtain the score on the first dimension directly from the svd decomposition reuslts using U[1,]*d[1], as shown in the following R code:
```{r}
svd_data1<-PCA(data1)
proj_data1_svd<-svd_data1$u[1,]*svd_data1$d[1]
cat("project the first experiment directly using svd ",proj_data1_svd[1],"\n")
cat("the value should be 1.481274 or -1.48127\n")
```

After projecting the data onto lower dimensional space, we can approximately recover the projected data in the original high dimensional space by projecting the projected, low dimensional data back using the principal components that are used previously to reduce the dimension. This operation is shown in Eq(5). The operation was implemented in the following R chunk by the function recoverData():

```{r}
recoverData<-function(Z,U,K){
  Z<-as.matrix(Z)
  U<-as.matrix(U)
  return(Z%*%t(U[,1:K]))
}
```
The following R code used the recoverData() function to project the data previously projected onto the first principal component back to the two dimensional space. The approximately recovered data for the first examle was printed out:

```{r}
rev_data1 <-recoverData(proj_data1,data1_U,1)
cat("Approximate of the first example is ",rev_data1[1,1],rev_data1[1,2],"\n")
```

To have a better understanding of how approxmate data points obtained from these projection-back projection procdure affect the original data points, the recovered data points using the first principal component, and the original data points were visualized in the following R chunk using the function plot_original_recovered_data():
```{r}
plot_original_recovered_data<-function(org_data,rev_data){
  m<-dim(org_data)[1]
  group_index<-c(1:m)
  
  col_index<-as.factor(c(rep(1,m),rep(2,m)))
  
  combined_data<-as.data.frame(rbind(cbind(org_data,group_index),cbind(rev_data,group_index)))
  ggplot(data=combined_data,aes(x=combined_data[,1],y=combined_data[,2],group=combined_data[,3]))+
    geom_point(color=col_index,shape=21,size=2)+geom_line(linetype="dashed")
  
}

plot_original_recovered_data(data1_norm,rev_data1)

```

In this plot, data points recovered from the first principal component were plotted in red, while the original data points were plotted in black. This figure showed that the recovered points are all aligned in the direction of the first principal component, and only information along the first principal component is retained. This figure shows that dimension reduction based on the first few principal compnents may result in the loss of the information carried by the principal components not included in the projection- backprojection process.

**2.4 Scree and cummulative PVE plots **                  
As shown in section 2.3, if only a few first principal components are selected in dimension reduction, we may lose some of the information in the original data. Therefore, defining the appropriate number of principal components  to maintain information in the original data with a few principal components is critical in dimension reduction.

The number of principal components used is determined based on the Proportion of Variance Explained (PVE) by each principal component. According to Eq(4), the variance a principal component can explain equals the eigenvalue corresponding to that principal component. Therefore, the PVE of a specific principal component $p_i$ is the ratio of $\lambda_i$ in the sum of all the $\lambda$ values, as expressed in the following equation:
$$PVE_i =\frac{\lambda_i}{\sum_{j=1}^K\lambda_j} ~~~~~~~~~~~~~Eq(6)$$
The following R chunk implements the function drawPVE() that plot both scree and cummulative PVE plots. Remember that $\lambda_i=d_i^2$ where $d_i$ is the i-th elment obtained from svd(X)$d, as discussed in section 1.2.3. Singular value decomposition:
```{r}
drawPVE<-function(data_X){
 
  lambdas<-PCA(data_X)$d^2
  lambdas<-lambdas/sum(lambdas)*100
  scree_data<-data.frame(x=seq_along(lambdas),y=lambdas,label=rep("scree",length(lambdas)))
  cumm_pve<-data.frame(x=seq_along(lambdas),y=cumsum(lambdas),label=rep("cummu PVE",length(lambdas)))
  
  g1<-ggplot(data=scree_data,aes(x=x,y=y))+geom_point(col="red")+geom_line(col="blue")+xlab("Principal Component")+ylab("PVE")
  g2<-ggplot(data=cumm_pve,aes(x=x,y=y))+geom_point(col="red")+ geom_line(col="blue")+xlab("Principal Component")+ylab("Cummulative PVE")
  grid.arrange(g1,g2, ncol=2)
  
  
}

```

**2.5. NCI60 data example **          
In this section, PCA is used on the NCI60 cancer cell line microarray data,  which consists of 6830 gene expression measurements on 64 cancer cell lines. The data is included in ISLR package. Let's first load and explore the data.
```{r}
nci.labs<-NCI60$labs
nci.data<-NCI60$data

cat("the length of nci.labs is ",length(nci.labs),"\n")
head(nci.labs)

cat("the dimension of nci.data is ",dim(nci.data),"\n")
nci.data[1:5,1:10]

```
nci.labs is a vector containing the cancer types of the 64 cell lines. nci.data is a 64 by 6830 matix. Each row corresponds to the 6830 gene expression measurements of one cell line. From the nci.labs, it would be helpful to explore the distribution of the cell lines versus the cancer types:
```{r}
table(nci.labs)
```
The table shows how many cell lines each cancer type contains in the dataset.

First, we will explore the data using the first few principal components for data visualization. We use 3 principal components, and plot the data scores pair-wise on 2D plots:
```{r}
cols<-function(vec){
  cols<-rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

pr<-PCA(nci.data)
pr.scores<-pr$u%*%diag(pr$d)
par(mfrow=c(1,2))
plot(x=pr.scores[,1],y=pr.scores[,2],col=cols(nci.labs),pch=19,xlab="Z1",ylab="Z2")
plot(x=pr.scores[,1],y=pr.scores[,3],col=cols(nci.labs),pch=19,xlab="Z1",ylab="Z3")
```
In these figures, observations belonging to the same cancer types are generally clustered together. Considering the huge amount of the features (6830 gene expression measurements), it would be impossible to visualize the data without using the dimension reduction method such as PCA.

Next, we would find the optimum number of principal components that we can use to represent the data in a lower dimension space using the drawPVE() function implemented in section 2.4 :
```{r}
drawPVE(nci.data)
```

From the scree plot on the left side, we can see an _elbow_ in the plot after the seventh principal component. This suggests that there may be little benefit to including more than 7 principal component for the analysis. From the plot on the right, the cummulative PVE shows that the first 7 principal components only explains about 40% of the variance. However, after the first 7 principal components, the PVE of each of the further principal components is very small.









